{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(629, 10)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv('tagged_dataset.csv', encoding='UTF-8').dropna()\n",
    "df = df[~df['genre'].isin(['NEWS-P4'])]  # remove invalid genre (only one document)\n",
    "df = shuffle(df, random_state=42)\n",
    "df['n_chars'] = df.tokens.apply(lambda x: len(x.split())) \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>genre</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>period</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>region</th>\n",
       "      <th>title</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "      <th>n_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>Aufforderung an die Herr Pfarrer und Schullehr...</td>\n",
       "      <td>P4</td>\n",
       "      <td>NN APPR ART NN NN KON NN $. PPER $, PRELS ART ...</td>\n",
       "      <td>OOD</td>\n",
       "      <td>Badisches</td>\n",
       "      <td>Aufforderung an die Herrn Pfarrer und Schulleh...</td>\n",
       "      <td>1832</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>SERM</td>\n",
       "      <td>so , mein Zuhörer , haben wir dies Augenblick ...</td>\n",
       "      <td>P5</td>\n",
       "      <td>ADV $, PPOSAT NN $, VAFIN PPER PDAT NN APPR AR...</td>\n",
       "      <td>NoD</td>\n",
       "      <td>Sonntag</td>\n",
       "      <td>So , meine Zuhörer , haben wir diesen Augenbli...</td>\n",
       "      <td>1861</td>\n",
       "      <td>2449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>Breslau von+die @card@ . Januar . gestern sein...</td>\n",
       "      <td>P4</td>\n",
       "      <td>NE APPRART CARD $. NN $. ADV VAFIN APPRART NN ...</td>\n",
       "      <td>OMD</td>\n",
       "      <td>Neue</td>\n",
       "      <td>Breslau vom 19 . Januar . Gestern war zur Feie...</td>\n",
       "      <td>1821</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>217</td>\n",
       "      <td>SERM</td>\n",
       "      <td>eine höchst bedeutungsvoll Fest sein es , mein...</td>\n",
       "      <td>P5</td>\n",
       "      <td>ART ADV ADJA NN VAFIN PPER $, PPOSAT NN $, PRE...</td>\n",
       "      <td>NoD</td>\n",
       "      <td>Gegenwärtige</td>\n",
       "      <td>Ein höchst bedeutungsvolles Fest ist es , mein...</td>\n",
       "      <td>1853</td>\n",
       "      <td>2450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>538</td>\n",
       "      <td>LEGA</td>\n",
       "      <td>die Polizei = Verordnung für Berlin . systemat...</td>\n",
       "      <td>P4</td>\n",
       "      <td>ART NN $( NN APPR NE $. ADJD VVPP APPR NE NE $...</td>\n",
       "      <td>NoD</td>\n",
       "      <td>DiePolizei=VerordnungfürBerlin</td>\n",
       "      <td>Die Polizei = Verordnungen für Berlin . System...</td>\n",
       "      <td>1850</td>\n",
       "      <td>2631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 genre                                             lemmas  \\\n",
       "500         500  NEWS  Aufforderung an die Herr Pfarrer und Schullehr...   \n",
       "248         248  SERM  so , mein Zuhörer , haben wir dies Augenblick ...   \n",
       "557         557  NEWS  Breslau von+die @card@ . Januar . gestern sein...   \n",
       "217         217  SERM  eine höchst bedeutungsvoll Fest sein es , mein...   \n",
       "538         538  LEGA  die Polizei = Verordnung für Berlin . systemat...   \n",
       "\n",
       "    period                                           pos_tags region  \\\n",
       "500     P4  NN APPR ART NN NN KON NN $. PPER $, PRELS ART ...    OOD   \n",
       "248     P5  ADV $, PPOSAT NN $, VAFIN PPER PDAT NN APPR AR...    NoD   \n",
       "557     P4  NE APPRART CARD $. NN $. ADV VAFIN APPRART NN ...    OMD   \n",
       "217     P5  ART ADV ADJA NN VAFIN PPER $, PPOSAT NN $, PRE...    NoD   \n",
       "538     P4  ART NN $( NN APPR NE $. ADJD VVPP APPR NE NE $...    NoD   \n",
       "\n",
       "                               title  \\\n",
       "500                        Badisches   \n",
       "248                          Sonntag   \n",
       "557                             Neue   \n",
       "217                    Gegenwärtige   \n",
       "538  DiePolizei=VerordnungfürBerlin   \n",
       "\n",
       "                                                tokens  year  n_chars  \n",
       "500  Aufforderung an die Herrn Pfarrer und Schulleh...  1832      494  \n",
       "248  So , meine Zuhörer , haben wir diesen Augenbli...  1861     2449  \n",
       "557  Breslau vom 19 . Januar . Gestern war zur Feie...  1821       51  \n",
       "217  Ein höchst bedeutungsvolles Fest ist es , mein...  1853     2450  \n",
       "538  Die Polizei = Verordnungen für Berlin . System...  1850     2631  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trennen von Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# tfidf values for tokens (n_grams)\n",
    "cv_token = TfidfVectorizer(stop_words=get_stop_words('de'), ngram_range=(1, 5))\n",
    "X_token_train = cv_token.fit_transform(df_train.tokens)\n",
    "X_token_test = cv_token.transform(df_test.tokens)\n",
    "\n",
    "# tfidf values for lemmas (n_grams)\n",
    "cv_lemma = TfidfVectorizer(stop_words=get_stop_words('de'))\n",
    "X_lemma_train = cv_lemma.fit_transform(df_train.lemmas)\n",
    "X_lemma_test = cv_lemma.transform(df_test.lemmas)\n",
    "\n",
    "# Only count pos tags\n",
    "cv_pos = CountVectorizer()\n",
    "X_pos_train = cv_pos.fit_transform(df_train.pos_tags)\n",
    "X_pos_test = cv_pos.transform(df_test.pos_tags)\n",
    "\n",
    "\n",
    "X_train = hstack([X_token_train, X_lemma_train, X_pos_train])\n",
    "X_test = hstack([X_token_test, X_lemma_test, X_pos_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lemma_pos = []\n",
    "#for index, row in df_train.iterrows():\n",
    "#    sample_lemma_pos = []\n",
    "#    for lemma, pos in zip(row.tokens, row.pos_tags):\n",
    "#        sample_lemma_pos.append(\"_\".join((lemma, pos)))\n",
    "#    train_lemma_pos.append(\" \".join(sample_lemma_pos))\n",
    "#\n",
    "#test_lemma_pos = []\n",
    "#for index, row in df_test.iterrows():\n",
    "#    sample_lemma_pos = []\n",
    "#    for lemma, pos in zip(row.tokens, row.pos_tags):\n",
    "#        sample_lemma_pos.append(\"_\".join((lemma, pos)))\n",
    "#    test_lemma_pos.append(\" \".join(sample_lemma_pos))\n",
    "#\n",
    "#merged_cv = TfidfVectorizer()\n",
    "#X_train = merged_cv.fit_transform(train_lemma_pos)\n",
    "#X_test = merged_cv.transform(test_lemma_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.genre\n",
    "y_test = df_test.genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((503, 1493997), (503,), (126, 1493997), (126,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       0.00      0.00      0.00         9\n",
      "        HUMA       0.00      0.00      0.00        18\n",
      "        LEGA       0.36      0.62      0.45        16\n",
      "        NARR       0.09      1.00      0.17         7\n",
      "        NEWS       1.00      0.13      0.23        53\n",
      "        SCIE       0.00      0.00      0.00        13\n",
      "        SERM       0.50      0.70      0.58        10\n",
      "\n",
      "    accuracy                           0.25       126\n",
      "   macro avg       0.28      0.35      0.21       126\n",
      "weighted avg       0.51      0.25      0.21       126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       1.00      1.00      1.00         9\n",
      "        HUMA       0.67      0.44      0.53        18\n",
      "        LEGA       0.92      0.75      0.83        16\n",
      "        NARR       0.67      0.86      0.75         7\n",
      "        NEWS       0.89      0.96      0.93        53\n",
      "        SCIE       0.60      0.69      0.64        13\n",
      "        SERM       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.83       126\n",
      "   macro avg       0.81      0.82      0.80       126\n",
      "weighted avg       0.83      0.83      0.83       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "linsvm_params = {\n",
    "    'C': [0.1, 0.5, 1, 1.5, 2, 3, 4]\n",
    "}\n",
    "\n",
    "gridsearch_linsvm = GridSearchCV(\n",
    "    LinearSVC(),\n",
    "    cv=5,\n",
    "    param_grid=linsvm_params,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gridsearch_linsvm.fit(X_train, y_train)\n",
    "gridsearch_linsvm.best_params_, gridsearch_linsvm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       1.00      0.89      0.94         9\n",
      "        HUMA       1.00      0.06      0.11        18\n",
      "        LEGA       1.00      0.50      0.67        16\n",
      "        NARR       0.33      0.86      0.48         7\n",
      "        NEWS       0.68      0.98      0.80        53\n",
      "        SCIE       0.67      0.15      0.25        13\n",
      "        SERM       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.68       126\n",
      "   macro avg       0.78      0.62      0.59       126\n",
      "weighted avg       0.78      0.68      0.62       126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linsvm = LinearSVC()\n",
    "linsvm.fit(X_train, y_train)\n",
    "y_pred = linsvm.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "rbfsvm = SVC(kernel='poly')\n",
    "rbfsvm.fit(X_train, y_train)\n",
    "y_pred = rbfsvm.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       1.00      0.89      0.94         9\n",
      "        HUMA       0.82      0.50      0.62        18\n",
      "        LEGA       0.93      0.81      0.87        16\n",
      "        NARR       0.40      0.57      0.47         7\n",
      "        NEWS       0.90      0.85      0.87        53\n",
      "        SCIE       0.42      0.77      0.54        13\n",
      "        SERM       0.78      0.70      0.74        10\n",
      "\n",
      "    accuracy                           0.76       126\n",
      "   macro avg       0.75      0.73      0.72       126\n",
      "weighted avg       0.81      0.76      0.77       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "y_pred = dectree.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       0.64      0.78      0.70         9\n",
      "        HUMA       0.86      0.33      0.48        18\n",
      "        LEGA       0.85      0.69      0.76        16\n",
      "        NARR       0.50      0.43      0.46         7\n",
      "        NEWS       0.74      0.98      0.85        53\n",
      "        SCIE       0.62      0.62      0.62        13\n",
      "        SERM       0.83      0.50      0.62        10\n",
      "\n",
      "    accuracy                           0.73       126\n",
      "   macro avg       0.72      0.62      0.64       126\n",
      "weighted avg       0.75      0.73      0.71       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randforest = RandomForestClassifier()\n",
    "randforest.fit(X_train, y_train)\n",
    "y_pred = randforest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       0.80      0.89      0.84         9\n",
      "        HUMA       0.56      0.28      0.37        18\n",
      "        LEGA       0.71      0.75      0.73        16\n",
      "        NARR       0.50      0.43      0.46         7\n",
      "        NEWS       0.87      0.75      0.81        53\n",
      "        SCIE       0.46      0.92      0.62        13\n",
      "        SERM       0.75      0.90      0.82        10\n",
      "\n",
      "    accuracy                           0.71       126\n",
      "   macro avg       0.66      0.70      0.66       126\n",
      "weighted avg       0.73      0.71      0.70       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgdsvm = SGDClassifier(loss='modified_huber', max_iter=15, random_state=42)\n",
    "\n",
    "sgdsvm.fit(X_train, y_train)\n",
    "y_pred = sgdsvm.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       0.00      0.00      0.00         9\n",
      "        HUMA       0.00      0.00      0.00        18\n",
      "        LEGA       0.28      1.00      0.44        16\n",
      "        NARR       0.14      0.43      0.21         7\n",
      "        NEWS       0.94      0.83      0.88        53\n",
      "        SCIE       0.00      0.00      0.00        13\n",
      "        SERM       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.50       126\n",
      "   macro avg       0.19      0.32      0.22       126\n",
      "weighted avg       0.44      0.50      0.44       126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    #base_estimator=SGDClassifier(loss='modified_huber', max_iter=5, random_state=42),\n",
    "    #algorithm='SAMME.R',\n",
    "    #n_estimators=100\n",
    ")\n",
    "\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4a7c856a95df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrad_boost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrad_boost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_boost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "grad_boost = XGBClassifier()\n",
    "grad_boost.fit(X_train, y_train)\n",
    "y_pred = grad_boost.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_boost_sklean = GradientBoostingClassifier()\n",
    "grad_boost_sklean.fit(X_train, y_train)\n",
    "y_pred = grad_boost_sklean.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tok = Tokenizer(num_words=20000)\n",
    "tok.fit_on_texts(df_train.text)\n",
    "\n",
    "Xk_train = tok.texts_to_matrix(df_train.text)\n",
    "Xk_test = tok.texts_to_matrix(df_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "yk_train = le.fit_transform(y_train)\n",
    "yk_test = le.transform(y_test)\n",
    "\n",
    "yk_train = to_categorical(yk_train)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Reshape\n",
    "\n",
    "def build_model(num_words, n_classes, hiddenlayer_size=512, n_hiddenlayer=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hiddenlayer_size, input_shape=(num_words, ), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    for i in range(n_hiddenlayer):\n",
    "        model.add(Dense(hiddenlayer_size, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = build_model(20000,\n",
    "                    len(np.unique(y_train)),\n",
    "                    n_hiddenlayer=3,\n",
    "                    hiddenlayer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Xk_train, yk_train,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yk_pred = model.predict_classes(X_test)\n",
    "print(classification_report(yk_test, yk_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "NUM_WORDS = 5000\n",
    "MAX_SEQ_LEN = 3000\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(df_train.text)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(df_train.text)\n",
    "test_sequences = tokenizer.texts_to_sequences(df_test.text)\n",
    "\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQ_LEN)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.data import Sentence\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def create_embedding_matrix(sequences, tokenizer: Tokenizer):\n",
    "    X = []\n",
    "    embedder = WordEmbeddings('de')\n",
    "    for sequence in train_sequences:\n",
    "        text = []\n",
    "        for entry in sequence:\n",
    "            text.append(tokenizer.index_word.get(entry, 'UNKOWN'))\n",
    "        text_mat = []\n",
    "        print(text[:4])\n",
    "        flair_data = Sentence(\" \".join(text))\n",
    "        embedder.embed(flair_data)\n",
    "        for token in flair_data:\n",
    "            text_mat.append(token.embedding.cpu().detach().numpy())\n",
    "        X.append(text_mat)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "def build_multiinput_model(embedding_dim, pos_input_shape, char_input_shape, num_classes):\n",
    "    \n",
    "    \"\"\"\n",
    "    Conceptional draft\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Input \n",
    "    embedding_input = Input(shape=(embedding_dim,))\n",
    "    emebedding_layer = Embedding(input_dim=embedding_dim, output_dim=100)(embedding_input)\n",
    "    embedding_conv_dropout = SpatialDropout1D(0.5)\n",
    "    embedding_conv = Conv1D(filters=128, kernel_size=(5,))(emebedding_layer)\n",
    "    \n",
    "    # 2. Input pos \n",
    "    pos_input = Input(shape=pos_input_shape)\n",
    "    pos_dense = Dense(512)(pos_input)\n",
    "    pos_dropout = Dropout(0.5)(pos_dense)\n",
    "    \n",
    "    # 3.Input char\n",
    "    char_input = Input(shape=char_input_shape)\n",
    "    char_embedding = Embedding(input_dim=embedding_dim, output_dim=100)(char_input) # meh\n",
    "    char_conv_dropout = SpatialDropout1D(0.5)\n",
    "    char_conv = Conv1D(filters=128, kernel_size=(5,))(char_embedding)\n",
    "\n",
    "    # 3. Concat input the three input layers\n",
    "    concat_layer = Concatenate()([embedding_conv, char_conv])\n",
    "    bi_lstm = Bidirectional(LSTM(16, return_sequences=True))(concat_layer)\n",
    "    flatten_layer = Flatten()(bi_lstm)\n",
    "    hidden_dense = Dense(512, activation='relu')(flatten_layer)\n",
    "    outpout_layer = Dense(num_classes, activation='softmax')\n",
    "    model = Model(inputs=[embedding_input, pos_input, char_input], outpouts=[outpout_layer])\n",
    "    mode.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 296, 128), (None, 96, 128)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-0f26bb3e36d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_multiinput_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-806b9feaa549>\u001b[0m in \u001b[0;36mbuild_multiinput_model\u001b[0;34m(embedding_dim, pos_input_shape, char_input_shape, num_classes)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# 3. Concat input the three input layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mconcat_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_conv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mbi_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mflatten_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbi_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m                              \u001b[0;34m'inputs with matching shapes '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                              \u001b[0;34m'except for the concat axis. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                              'Got inputs shapes: %s' % (input_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 296, 128), (None, 96, 128)]"
     ]
    }
   ],
   "source": [
    "model = build_multiinput_model(300, (5000,), (100,), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
