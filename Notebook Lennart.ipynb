{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(629, 9)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv('tagged_dataset.csv', encoding='UTF-8').dropna()\n",
    "df = df[~df['genre'].isin(['NEWS-P4'])]  # remove invalid genre (only one document)\n",
    "df = shuffle(df, random_state=42)\n",
    "#df['n_chars'] = df.tokens.apply(lambda x: len(x.split())) \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>genre</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>period</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>region</th>\n",
       "      <th>title</th>\n",
       "      <th>tokens</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>Aufforderung an die Herr Pfarrer und Schullehr...</td>\n",
       "      <td>P4</td>\n",
       "      <td>NN APPR ART NN NN KON NN $. PPER $, PRELS ART ...</td>\n",
       "      <td>OOD</td>\n",
       "      <td>Badisches</td>\n",
       "      <td>Aufforderung an die Herrn Pfarrer und Schulleh...</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>SERM</td>\n",
       "      <td>so , mein Zuhörer , haben wir dies Augenblick ...</td>\n",
       "      <td>P5</td>\n",
       "      <td>ADV $, PPOSAT NN $, VAFIN PPER PDAT NN APPR AR...</td>\n",
       "      <td>NoD</td>\n",
       "      <td>Sonntag</td>\n",
       "      <td>So , meine Zuhörer , haben wir diesen Augenbli...</td>\n",
       "      <td>1861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>Breslau von+die @card@ . Januar . gestern sein...</td>\n",
       "      <td>P4</td>\n",
       "      <td>NE APPRART CARD $. NN $. ADV VAFIN APPRART NN ...</td>\n",
       "      <td>OMD</td>\n",
       "      <td>Neue</td>\n",
       "      <td>Breslau vom 19 . Januar . Gestern war zur Feie...</td>\n",
       "      <td>1821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>217</td>\n",
       "      <td>SERM</td>\n",
       "      <td>eine höchst bedeutungsvoll Fest sein es , mein...</td>\n",
       "      <td>P5</td>\n",
       "      <td>ART ADV ADJA NN VAFIN PPER $, PPOSAT NN $, PRE...</td>\n",
       "      <td>NoD</td>\n",
       "      <td>Gegenwärtige</td>\n",
       "      <td>Ein höchst bedeutungsvolles Fest ist es , mein...</td>\n",
       "      <td>1853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>538</td>\n",
       "      <td>LEGA</td>\n",
       "      <td>die Polizei = Verordnung für Berlin . systemat...</td>\n",
       "      <td>P4</td>\n",
       "      <td>ART NN $( NN APPR NE $. ADJD VVPP APPR NE NE $...</td>\n",
       "      <td>NoD</td>\n",
       "      <td>DiePolizei=VerordnungfürBerlin</td>\n",
       "      <td>Die Polizei = Verordnungen für Berlin . System...</td>\n",
       "      <td>1850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 genre                                             lemmas  \\\n",
       "500         500  NEWS  Aufforderung an die Herr Pfarrer und Schullehr...   \n",
       "248         248  SERM  so , mein Zuhörer , haben wir dies Augenblick ...   \n",
       "557         557  NEWS  Breslau von+die @card@ . Januar . gestern sein...   \n",
       "217         217  SERM  eine höchst bedeutungsvoll Fest sein es , mein...   \n",
       "538         538  LEGA  die Polizei = Verordnung für Berlin . systemat...   \n",
       "\n",
       "    period                                           pos_tags region  \\\n",
       "500     P4  NN APPR ART NN NN KON NN $. PPER $, PRELS ART ...    OOD   \n",
       "248     P5  ADV $, PPOSAT NN $, VAFIN PPER PDAT NN APPR AR...    NoD   \n",
       "557     P4  NE APPRART CARD $. NN $. ADV VAFIN APPRART NN ...    OMD   \n",
       "217     P5  ART ADV ADJA NN VAFIN PPER $, PPOSAT NN $, PRE...    NoD   \n",
       "538     P4  ART NN $( NN APPR NE $. ADJD VVPP APPR NE NE $...    NoD   \n",
       "\n",
       "                               title  \\\n",
       "500                        Badisches   \n",
       "248                          Sonntag   \n",
       "557                             Neue   \n",
       "217                    Gegenwärtige   \n",
       "538  DiePolizei=VerordnungfürBerlin   \n",
       "\n",
       "                                                tokens  year  \n",
       "500  Aufforderung an die Herrn Pfarrer und Schulleh...  1832  \n",
       "248  So , meine Zuhörer , haben wir diesen Augenbli...  1861  \n",
       "557  Breslau vom 19 . Januar . Gestern war zur Feie...  1821  \n",
       "217  Ein höchst bedeutungsvolles Fest ist es , mein...  1853  \n",
       "538  Die Polizei = Verordnungen für Berlin . System...  1850  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trennen von Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# tfidf values for tokens (n_grams)\n",
    "cv_token = TfidfVectorizer(stop_words=get_stop_words('de'), max_features=100)\n",
    "X_token_train = cv_token.fit_transform(df_train.tokens)\n",
    "X_token_test = cv_token.transform(df_test.tokens)\n",
    "\n",
    "# tfidf values for lemmas (n_grams)\n",
    "cv_lemma = TfidfVectorizer(max_features=6000)#stop_words=get_stop_words('de'),  \n",
    "X_lemma_train = cv_lemma.fit_transform(df_train.lemmas)\n",
    "X_lemma_test = cv_lemma.transform(df_test.lemmas)\n",
    "\n",
    "# Only count pos tags\n",
    "cv_pos = CountVectorizer()\n",
    "X_pos_train = cv_pos.fit_transform(df_train.pos_tags)\n",
    "X_pos_test = cv_pos.transform(df_test.pos_tags)\n",
    "\n",
    "# Concatenate features horizontally\n",
    "features = np.hstack([\n",
    "    np.array(list(cv_token.vocabulary_.keys())),\n",
    "    np.array(list(cv_lemma.vocabulary_.keys())),\n",
    "    np.array(list(cv_pos.vocabulary_.keys()))\n",
    "])\n",
    "\n",
    "X_train = hstack([\n",
    "    X_token_train,\n",
    "    X_lemma_train,\n",
    "    X_pos_train\n",
    "])\n",
    "X_test = hstack([\n",
    "    X_token_test,\n",
    "    X_lemma_test,\n",
    "    X_pos_test\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lemma_pos = []\n",
    "#for index, row in df_train.iterrows():\n",
    "#    sample_lemma_pos = []\n",
    "#    for lemma, pos in zip(row.tokens, row.pos_tags):\n",
    "#        sample_lemma_pos.append(\"_\".join((lemma, pos)))\n",
    "#    train_lemma_pos.append(\" \".join(sample_lemma_pos))\n",
    "#\n",
    "#test_lemma_pos = []\n",
    "#for index, row in df_test.iterrows():\n",
    "#    sample_lemma_pos = []\n",
    "#    for lemma, pos in zip(row.tokens, row.pos_tags):\n",
    "#        sample_lemma_pos.append(\"_\".join((lemma, pos)))\n",
    "#    test_lemma_pos.append(\" \".join(sample_lemma_pos))\n",
    "#\n",
    "#merged_cv = TfidfVectorizer()\n",
    "#X_train = merged_cv.fit_transform(train_lemma_pos)\n",
    "#X_test = merged_cv.transform(test_lemma_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.genre\n",
    "y_test = df_test.genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6150,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((503, 6150), (503,), (126, 6150), (126,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       1.00      1.00      1.00         9\n",
      "        HUMA       0.50      0.11      0.18        18\n",
      "        LEGA       0.65      0.69      0.67        16\n",
      "        NARR       0.50      0.86      0.63         7\n",
      "        NEWS       0.86      0.70      0.77        53\n",
      "        SCIE       0.38      0.85      0.52        13\n",
      "        SERM       0.83      1.00      0.91        10\n",
      "\n",
      "    accuracy                           0.68       126\n",
      "   macro avg       0.67      0.74      0.67       126\n",
      "weighted avg       0.72      0.68      0.67       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       1.00      1.00      1.00         9\n",
      "        HUMA       0.64      0.39      0.48        18\n",
      "        LEGA       0.86      0.75      0.80        16\n",
      "        NARR       0.67      0.86      0.75         7\n",
      "        NEWS       0.89      0.96      0.93        53\n",
      "        SCIE       0.60      0.69      0.64        13\n",
      "        SERM       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.83       126\n",
      "   macro avg       0.79      0.81      0.79       126\n",
      "weighted avg       0.82      0.83      0.82       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check feature weights of logreg model to get highly correlated features for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: DRAM\n",
      "Positive features\n",
      "deutschen 2.906600954330881\n",
      "gibt 1.8768686249087378\n",
      "mutter 1.5450681232860786\n",
      "viele 1.510233465633998\n",
      "wegen 1.3856997124137145\n",
      "seit 1.2923487252888761\n",
      "sei 1.244900592196551\n",
      "sehen 1.150804129548595\n",
      "bald 1.0005089768352733\n",
      "fu 0.8786443948747108\n",
      "\n",
      "Negative features\n",
      "jahre -1.6894873237345962\n",
      "alte -1.129915568158518\n",
      "große -1.123023466867393\n",
      "seyn -0.8919906474551318\n",
      "herr -0.8850382269963954\n",
      "schon -0.8816901786342032\n",
      "einzelnen -0.8718448481385498\n",
      "wohl -0.8279963415790786\n",
      "zwei -0.8129910947501675\n",
      "tag -0.7934239046229704\n",
      "\n",
      "########################################\n",
      "Class: HUMA\n",
      "Positive features\n",
      "gott 1.8795639257345078\n",
      "herrn 1.678565881622185\n",
      "wasser 1.2952886717343095\n",
      "wer 1.0981184654156566\n",
      "wort 1.0935045748553873\n",
      "ja 1.0627937415708977\n",
      "hand 0.9342812250228008\n",
      "tag 0.8374884853422764\n",
      "ganz 0.8169209603975234\n",
      "weiß 0.8065272083605995\n",
      "\n",
      "Negative features\n",
      "geist -1.4121976069818587\n",
      "muß -1.137299419198156\n",
      "tage -1.0824044196211868\n",
      "stehen -0.7754124243062941\n",
      "weit -0.7403190031724705\n",
      "ersten -0.7304088645744299\n",
      "zeit -0.6604857683618006\n",
      "drei -0.6376337853618234\n",
      "seit -0.6339553794439398\n",
      "beim -0.622328280817721\n",
      "\n",
      "########################################\n",
      "Class: LEGA\n",
      "Positive features\n",
      "einzelnen 2.843761648489627\n",
      "gut 2.05211152963093\n",
      "erst 1.4873582931580835\n",
      "herr 1.3870267844501405\n",
      "gott 1.3004864410837247\n",
      "daher 1.2409365142648974\n",
      "beim 1.0845016816125592\n",
      "herrn 0.9894570918761224\n",
      "worden 0.9022650523155809\n",
      "zwei 0.8853322443943052\n",
      "\n",
      "Negative features\n",
      "jahre -1.8350441959209023\n",
      "deutschen -0.957538467134881\n",
      "oft -0.9419947995756548\n",
      "fu -0.9181798352671228\n",
      "wurde -0.915524061334276\n",
      "alten -0.8582266393097946\n",
      "menschen -0.8540502742177215\n",
      "art -0.8205497085427461\n",
      "schon -0.8161509629650978\n",
      "tage -0.8153888392235302\n",
      "\n",
      "########################################\n",
      "Class: NARR\n",
      "Positive features\n",
      "gar 2.446592323322936\n",
      "luft 1.9585878800352423\n",
      "jedoch 1.5287722831731507\n",
      "geht 1.4854737436400347\n",
      "tage 1.4379748282223301\n",
      "wort 1.3992174598875196\n",
      "schon 1.3052017886120175\n",
      "seit 1.2758004229103053\n",
      "ganzen 1.2730124187734475\n",
      "müssen 1.2279023729693859\n",
      "\n",
      "Negative features\n",
      "alte -0.8473943335494926\n",
      "herrn -0.8351398323696401\n",
      "worden -0.735629755161037\n",
      "wurden -0.7182960794846293\n",
      "hand -0.6950663142455119\n",
      "erst -0.6783801051167013\n",
      "viele -0.6567850839276038\n",
      "muß -0.6463057446901458\n",
      "ko -0.626645442193222\n",
      "einzelnen -0.6222483481563841\n",
      "\n",
      "########################################\n",
      "Class: NEWS\n",
      "Positive features\n",
      "jahre 2.7034976925043623\n",
      "weise 1.0515454430506954\n",
      "alten 0.9823721625206114\n",
      "ko 0.9402901140311295\n",
      "alte 0.9257554758815231\n",
      "immer 0.7547442518189323\n",
      "geist 0.7040041830614798\n",
      "muß 0.6463232719187937\n",
      "beiden 0.6183618063719911\n",
      "geben 0.4282704965364594\n",
      "\n",
      "Negative features\n",
      "oft -2.2153933685289062\n",
      "deutschen -1.9557525257857855\n",
      "einzelnen -1.9028130521454198\n",
      "konnte -1.5188757487778286\n",
      "gott -1.5084495773196123\n",
      "seit -1.4779969388336764\n",
      "große -1.446332187358414\n",
      "kommt -1.4411559260080695\n",
      "luft -1.4175790298833135\n",
      "drei -1.3852060125635732\n",
      "\n",
      "########################################\n",
      "Class: SCIE\n",
      "Positive features\n",
      "drei 2.5699062976668725\n",
      "denen 1.6041370675779933\n",
      "hand 1.5832959768357326\n",
      "wohl 1.507044422420953\n",
      "lange 1.172064560732099\n",
      "ganze 1.0644857243594623\n",
      "stadt 1.0504447207285912\n",
      "namen 0.9872954578160653\n",
      "art 0.9841020387675363\n",
      "lassen 0.9778699809720057\n",
      "\n",
      "Negative features\n",
      "weise -1.3224543645483546\n",
      "geist -1.2524982396620523\n",
      "oft -1.1145006616059059\n",
      "jahre -1.0057923431825966\n",
      "mann -0.9690998023155925\n",
      "gott -0.9201172579928031\n",
      "jedoch -0.8851620047034918\n",
      "tage -0.8826823220563444\n",
      "weit -0.8475227817696552\n",
      "seit -0.8316595324432783\n",
      "\n",
      "########################################\n",
      "Class: SERM\n",
      "Positive features\n",
      "oft 3.271727422647167\n",
      "große 2.9256349082523143\n",
      "muß 1.647019371950065\n",
      "je 1.2650253960689208\n",
      "ja 1.2466839062061654\n",
      "deren 1.1497402834480415\n",
      "augen 1.0496488616034587\n",
      "weiß 1.0233712435948954\n",
      "art 0.9467069174368324\n",
      "seyn 0.8645214117549989\n",
      "\n",
      "Negative features\n",
      "jahre -1.1674472677507943\n",
      "tage -1.0162103561490963\n",
      "wurde -0.9430906271328896\n",
      "luft -0.793924405965543\n",
      "gut -0.7675846089462165\n",
      "sei -0.7480184989980584\n",
      "alte -0.6961685608561512\n",
      "jedoch -0.6766965474865896\n",
      "wohl -0.6448597093190893\n",
      "vater -0.5922609846180298\n",
      "\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "for class_ind, class_coef in enumerate(logreg.coef_):\n",
    "    print('Class:', logreg.classes_[class_ind])\n",
    "    print('Positive features')\n",
    "    positive_features = np.flip(np.argsort(class_coef)[-n:])\n",
    "    for ind in positive_features:\n",
    "        print(features[ind], class_coef[ind])\n",
    "    print()\n",
    "    print('Negative features')\n",
    "    negative_features = np.argsort(class_coef)[:n]\n",
    "    for ind in negative_features:\n",
    "        print(features[ind], class_coef[ind])\n",
    "    print()\n",
    "    print('#'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "linsvm_params = {\n",
    "    'C': [0.1, 0.5, 1, 1.5, 2, 3, 4]\n",
    "}\n",
    "\n",
    "gridsearch_linsvm = GridSearchCV(\n",
    "    LinearSVC(),\n",
    "    cv=5,\n",
    "    param_grid=linsvm_params,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gridsearch_linsvm.fit(X_train, y_train)\n",
    "gridsearch_linsvm.best_params_, gridsearch_linsvm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       0.90      1.00      0.95         9\n",
      "        HUMA       0.75      0.50      0.60        18\n",
      "        LEGA       1.00      0.50      0.67        16\n",
      "        NARR       0.40      0.86      0.55         7\n",
      "        NEWS       0.74      1.00      0.85        53\n",
      "        SCIE       1.00      0.08      0.14        13\n",
      "        SERM       0.88      0.70      0.78        10\n",
      "\n",
      "    accuracy                           0.74       126\n",
      "   macro avg       0.81      0.66      0.65       126\n",
      "weighted avg       0.80      0.74      0.70       126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linsvm = LinearSVC()\n",
    "linsvm.fit(X_train, y_train)\n",
    "y_pred = linsvm.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "rbfsvm = SVC(kernel='poly')\n",
    "rbfsvm.fit(X_train, y_train)\n",
    "y_pred = rbfsvm.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       1.00      0.89      0.94         9\n",
      "        HUMA       0.71      0.56      0.63        18\n",
      "        LEGA       0.76      0.81      0.79        16\n",
      "        NARR       0.50      0.71      0.59         7\n",
      "        NEWS       0.94      0.85      0.89        53\n",
      "        SCIE       0.45      0.69      0.55        13\n",
      "        SERM       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.78       126\n",
      "   macro avg       0.75      0.76      0.75       126\n",
      "weighted avg       0.81      0.78      0.79       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "y_pred = dectree.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       0.88      0.78      0.82         9\n",
      "        HUMA       0.54      0.39      0.45        18\n",
      "        LEGA       0.87      0.81      0.84        16\n",
      "        NARR       0.40      0.29      0.33         7\n",
      "        NEWS       0.89      0.91      0.90        53\n",
      "        SCIE       0.60      0.69      0.64        13\n",
      "        SERM       0.62      1.00      0.77        10\n",
      "\n",
      "    accuracy                           0.76       126\n",
      "   macro avg       0.68      0.69      0.68       126\n",
      "weighted avg       0.76      0.76      0.75       126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randforest = RandomForestClassifier()\n",
    "randforest.fit(X_train, y_train)\n",
    "y_pred = randforest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       0.80      0.89      0.84         9\n",
      "        HUMA       0.56      0.28      0.37        18\n",
      "        LEGA       0.71      0.75      0.73        16\n",
      "        NARR       0.50      0.43      0.46         7\n",
      "        NEWS       0.87      0.75      0.81        53\n",
      "        SCIE       0.46      0.92      0.62        13\n",
      "        SERM       0.75      0.90      0.82        10\n",
      "\n",
      "    accuracy                           0.71       126\n",
      "   macro avg       0.66      0.70      0.66       126\n",
      "weighted avg       0.73      0.71      0.70       126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgdsvm = SGDClassifier(loss='modified_huber', max_iter=15, random_state=42)\n",
    "\n",
    "sgdsvm.fit(X_train, y_train)\n",
    "y_pred = sgdsvm.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       0.00      0.00      0.00         9\n",
      "        HUMA       0.00      0.00      0.00        18\n",
      "        LEGA       0.28      1.00      0.44        16\n",
      "        NARR       0.14      0.43      0.21         7\n",
      "        NEWS       0.94      0.83      0.88        53\n",
      "        SCIE       0.00      0.00      0.00        13\n",
      "        SERM       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.50       126\n",
      "   macro avg       0.19      0.32      0.22       126\n",
      "weighted avg       0.44      0.50      0.44       126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennartkeller/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    #base_estimator=SGDClassifier(loss='modified_huber', max_iter=5, random_state=42),\n",
    "    #algorithm='SAMME.R',\n",
    "    #n_estimators=100\n",
    ")\n",
    "\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4a7c856a95df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrad_boost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrad_boost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_boost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "grad_boost = XGBClassifier()\n",
    "grad_boost.fit(X_train, y_train)\n",
    "y_pred = grad_boost.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boostrap Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def bootstrap_validation(clf1: BaseEstimator, clf2: BaseEstimator,\n",
    "                         X: csr_matrix, y: csr_matrix,\n",
    "                         n_samples: int,\n",
    "                         sample_size: int,\n",
    "                         scorer: callable = lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro')) -> float:\n",
    "    \n",
    "    y_pred_1 = clf1.predict(X)\n",
    "    clf1_score = scorer(y, y_pred_1)\n",
    "    \n",
    "    y_pred_2 = clf2.predict(X)\n",
    "    clf2_score = scorer(y, y_pred_2)\n",
    "    \n",
    "    initial_difference = clf1_score - clf2_score\n",
    "    \n",
    "    sample_differences = []\n",
    "    for _ in range(n_samples):\n",
    "        \n",
    "        # create boostrap sample\n",
    "        X_sample, y_sample = resample(X, y, replace=True, n_samples=sample_size)\n",
    "        # calculate performance difference and store it\n",
    "        y_pred_1 = clf1.predict(X_sample)\n",
    "        clf1_score = scorer(y_sample, y_pred_1)\n",
    "    \n",
    "        y_pred_2 = clf2.predict(X_sample)\n",
    "        clf2_score = scorer(y_sample, y_pred_2)\n",
    "        sample_differences.append(clf1_score - clf2_score)\n",
    "        \n",
    "    \n",
    "    # calculate p value based on performance differences\n",
    "    s_values = []\n",
    "    s = 0\n",
    "    for sample_difference in sample_differences:\n",
    "        if sample_difference > 2 * initial_difference:\n",
    "            s += 1\n",
    "    p_value = s / n_samples\n",
    "    return s, p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4533333333333333 0.35333333333333333\n",
      "0.5 0.480952380952381\n",
      "0.71 0.5416666666666666\n",
      "0.3928571428571429 0.2704761904761905\n",
      "0.3444444444444444 0.3444444444444444\n",
      "0.6011904761904762 0.7166666666666667\n",
      "0.4 0.4333333333333333\n",
      "0.373015873015873 0.25142857142857145\n",
      "0.798095238095238 0.9314285714285713\n",
      "0.38888888888888884 0.5111111111111111\n",
      "0.3888888888888889 0.5222222222222223\n",
      "0.7083333333333334 0.6416666666666666\n",
      "0.3333333333333333 0.3642857142857143\n",
      "0.4375 0.4375\n",
      "0.44761904761904764 0.3722222222222222\n",
      "0.75 0.6666666666666666\n",
      "0.3333333333333333 0.39285714285714285\n",
      "0.6507936507936508 0.6507936507936508\n",
      "0.3095238095238095 0.5238095238095238\n",
      "0.74 0.8266666666666665\n",
      "0.3333333333333333 0.2333333333333333\n",
      "0.5599999999999999 0.6476190476190476\n",
      "0.6714285714285715 0.6166666666666666\n",
      "0.44761904761904764 0.6517857142857143\n",
      "0.42777777777777776 0.31666666666666665\n",
      "0.45999999999999996 0.6266666666666667\n",
      "0.5666666666666667 0.5777777777777778\n",
      "0.4740740740740741 0.41190476190476194\n",
      "0.71 0.8314285714285713\n",
      "0.4388888888888889 0.49444444444444446\n",
      "0.6166666666666667 0.45999999999999996\n",
      "0.5277777777777778 0.4244897959183674\n",
      "0.72 0.72\n",
      "0.39047619047619053 0.4333333333333333\n",
      "0.7133333333333333 0.6063492063492063\n",
      "0.6428571428571429 0.6428571428571429\n",
      "0.47222222222222215 0.48333333333333334\n",
      "0.43809523809523804 0.6777777777777777\n",
      "0.8472222222222222 0.7414965986394557\n",
      "0.75 0.75\n",
      "0.6933333333333334 0.6666666666666666\n",
      "0.5666666666666665 0.5666666666666665\n",
      "0.26666666666666666 0.4600000000000001\n",
      "0.43333333333333335 0.41111111111111115\n",
      "0.5133333333333333 0.4133333333333333\n",
      "0.6266666666666667 0.5933333333333334\n",
      "0.3090909090909091 0.553030303030303\n",
      "0.5266666666666666 0.3904761904761904\n",
      "0.4444444444444445 0.7333333333333334\n",
      "0.38181818181818183 0.56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 0.3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# train simple sgd with hinge loss => linear support vector machine\n",
    "# train simple sgd with log loss => logistic regression\n",
    "\n",
    "sgd_hinge = SGDClassifier(loss='hinge')\n",
    "sgd_hinge.fit(X_train, y_train)\n",
    "\n",
    "sgd_log = SGDClassifier(loss='log')\n",
    "sgd_log.fit(X_train, y_train)\n",
    "bootstrap_validation(sgd_hinge, sgd_log, X_test, y_test, 50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DRAM       0.88      0.78      0.82         9\n",
      "        HUMA       0.93      0.78      0.85        18\n",
      "        LEGA       0.94      1.00      0.97        16\n",
      "        NARR       0.67      0.86      0.75         7\n",
      "        NEWS       0.93      0.96      0.94        53\n",
      "        SCIE       0.92      0.92      0.92        13\n",
      "        SERM       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.91       126\n",
      "   macro avg       0.90      0.89      0.89       126\n",
      "weighted avg       0.92      0.91      0.91       126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_boost_sklean = GradientBoostingClassifier()\n",
    "grad_boost_sklean.fit(X_train, y_train)\n",
    "y_pred = grad_boost_sklean.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tok = Tokenizer(num_words=20000)\n",
    "tok.fit_on_texts(df_train.text)\n",
    "\n",
    "Xk_train = tok.texts_to_matrix(df_train.text)\n",
    "Xk_test = tok.texts_to_matrix(df_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "yk_train = le.fit_transform(y_train)\n",
    "yk_test = le.transform(y_test)\n",
    "\n",
    "yk_train = to_categorical(yk_train)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Reshape\n",
    "\n",
    "def build_model(num_words, n_classes, hiddenlayer_size=512, n_hiddenlayer=1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hiddenlayer_size, input_shape=(num_words, ), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    for i in range(n_hiddenlayer):\n",
    "        model.add(Dense(hiddenlayer_size, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = build_model(20000,\n",
    "                    len(np.unique(y_train)),\n",
    "                    n_hiddenlayer=3,\n",
    "                    hiddenlayer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(Xk_train, yk_train,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yk_pred = model.predict_classes(X_test)\n",
    "print(classification_report(yk_test, yk_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "NUM_WORDS = 5000\n",
    "MAX_SEQ_LEN = 3000\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(df_train.text)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(df_train.text)\n",
    "test_sequences = tokenizer.texts_to_sequences(df_test.text)\n",
    "\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQ_LEN)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "from flair.data import Sentence\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def create_embedding_matrix(sequences, tokenizer: Tokenizer):\n",
    "    X = []\n",
    "    embedder = WordEmbeddings('de')\n",
    "    for sequence in train_sequences:\n",
    "        text = []\n",
    "        for entry in sequence:\n",
    "            text.append(tokenizer.index_word.get(entry, 'UNKOWN'))\n",
    "        text_mat = []\n",
    "        print(text[:4])\n",
    "        flair_data = Sentence(\" \".join(text))\n",
    "        embedder.embed(flair_data)\n",
    "        for token in flair_data:\n",
    "            text_mat.append(token.embedding.cpu().detach().numpy())\n",
    "        X.append(text_mat)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "def build_multiinput_model(embedding_dim, pos_input_shape, char_input_shape, num_classes):\n",
    "    \n",
    "    \"\"\"\n",
    "    Conceptional draft\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Input \n",
    "    embedding_input = Input(shape=(embedding_dim,))\n",
    "    emebedding_layer = Embedding(input_dim=embedding_dim, output_dim=100)(embedding_input)\n",
    "    embedding_conv_dropout = SpatialDropout1D(0.5)\n",
    "    embedding_conv = Conv1D(filters=128, kernel_size=(5,))(emebedding_layer)\n",
    "    \n",
    "    # 2. Input pos \n",
    "    pos_input = Input(shape=pos_input_shape)\n",
    "    pos_dense = Dense(512)(pos_input)\n",
    "    pos_dropout = Dropout(0.5)(pos_dense)\n",
    "    \n",
    "    # 3.Input char\n",
    "    char_input = Input(shape=char_input_shape)\n",
    "    char_embedding = Embedding(input_dim=embedding_dim, output_dim=100)(char_input) # meh\n",
    "    char_conv_dropout = SpatialDropout1D(0.5)\n",
    "    char_conv = Conv1D(filters=128, kernel_size=(5,))(char_embedding)\n",
    "\n",
    "    # 3. Concat input the three input layers\n",
    "    concat_layer = Concatenate()([embedding_conv, char_conv])\n",
    "    bi_lstm = Bidirectional(LSTM(16, return_sequences=True))(concat_layer)\n",
    "    flatten_layer = Flatten()(bi_lstm)\n",
    "    hidden_dense = Dense(512, activation='relu')(flatten_layer)\n",
    "    outpout_layer = Dense(num_classes, activation='softmax')\n",
    "    model = Model(inputs=[embedding_input, pos_input, char_input], outpouts=[outpout_layer])\n",
    "    mode.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 296, 128), (None, 96, 128)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-0f26bb3e36d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_multiinput_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-806b9feaa549>\u001b[0m in \u001b[0;36mbuild_multiinput_model\u001b[0;34m(embedding_dim, pos_input_shape, char_input_shape, num_classes)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# 3. Concat input the three input layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mconcat_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_conv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mbi_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mflatten_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbi_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m                              \u001b[0;34m'inputs with matching shapes '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                              \u001b[0;34m'except for the concat axis. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                              'Got inputs shapes: %s' % (input_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 296, 128), (None, 96, 128)]"
     ]
    }
   ],
   "source": [
    "model = build_multiinput_model(300, (5000,), (100,), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
