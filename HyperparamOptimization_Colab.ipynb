{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:nlp]",
      "language": "python",
      "name": "conda-env-nlp-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "HyperparamOptimization.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LennartKeller/TextklassifikationsProjekt2019/blob/master/HyperparamOptimization_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLxGUx5_Otq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Union, List\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.exceptions import NotFittedError\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class PeriodEstimatorWrapper(BaseEstimator):\n",
        "\n",
        "    def __init__(self, clf: BaseEstimator, **params):\n",
        "        self.clf = clf(**params)\n",
        "        if params.get('verbose'):\n",
        "            self.verbose = params['verbose']\n",
        "\n",
        "    def fit(self, X_train: Union[csr_matrix, np.ndarray], y_train: np.array):\n",
        "        \"\"\"\n",
        "        Fits the estimator.\n",
        "\n",
        "        :param X_train: normal feature matrix e.g. shape (n_samples, n_features)\n",
        "        :param y_train: label vector shape (n_samples,)\n",
        "        :return: fitted instance of itself\n",
        "        \"\"\"\n",
        "\n",
        "        self.clf.fit(X_train, y_train)\n",
        "        self.fitted_ = True\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_test: List[Union[csr_matrix, np.ndarray]]):\n",
        "        \"\"\"\n",
        "        Predicts classes for n periods\n",
        "        :param X_test: list of feature matrices (n_samples, n_features) to predict (one for each period)\n",
        "        :return: list of predicted label vectors\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.fitted_:\n",
        "            raise NotFittedError\n",
        "\n",
        "        result = []\n",
        "        if self.verbose:\n",
        "            iterator = tqdm(X_test, desc='Predicting classes for periods')\n",
        "        else:\n",
        "            iterator = X_test\n",
        "\n",
        "        for X in iterator:\n",
        "            result.append(self.clf.predict(X))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def predict_proba(self, X_test: List[Union[csr_matrix, np.ndarray]]):\n",
        "        \"\"\"\n",
        "        Predicts probabilities for n periods\n",
        "        :param X_test: list of feature matrices (n_samples, n_features) to predict (one for each period)\n",
        "        :return: list of predicted label vectors\n",
        "        \"\"\"\n",
        "        if not hasattr(self.clf, 'predict_proba'):\n",
        "            raise Exception(f\"Method predict_proba is not implemented in {self.clf.__class__.__name__}\")\n",
        "\n",
        "        if not self.fitted_:\n",
        "            raise NotFittedError\n",
        "\n",
        "        result = []\n",
        "        if self.verbose:\n",
        "            iterator = tqdm(X_test, desc='Predicting classes for periods')\n",
        "        else:\n",
        "            iterator = X_test\n",
        "\n",
        "        for X in iterator:\n",
        "            result.append(self.clf.predict_proba(X))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def decision_function(self, X_test: List[Union[csr_matrix, np.ndarray]]):\n",
        "        \"\"\"\n",
        "        Predicts decision scores for n periods\n",
        "        :param X_test: list of feature matrices (n_samples, n_features) to predict (one for each period)\n",
        "        :return: list of predicted label vectors\n",
        "        \"\"\"\n",
        "        if not hasattr(self.clf, 'decision_function'):\n",
        "            raise Exception(f\"Method decision_function is not implemented in {self.clf.__class__.__name__}\")\n",
        "\n",
        "        if not self.fitted_:\n",
        "            raise NotFittedError\n",
        "\n",
        "        result = []\n",
        "        if self.verbose:\n",
        "            iterator = tqdm(X_test, desc='Predicting classes for periods')\n",
        "        else:\n",
        "            iterator = X_test\n",
        "\n",
        "        for X in iterator:\n",
        "            result.append(self.clf.predict_proba(X))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def score(self,\n",
        "              X_test: List[Union[csr_matrix, np.ndarray]],\n",
        "              y_true: List[np.array],\n",
        "              scoring_func: callable = lambda y_true, y_pred: f1_score(y_true, y_pred, average='macro'),\n",
        "              pooling_func: callable = np.mean):\n",
        "\n",
        "        if not self.fitted_:\n",
        "            raise NotFittedError\n",
        "\n",
        "        scores = []\n",
        "        for X, y in zip(X_test, y_true):\n",
        "            y_pred = self.clf.predict(X)\n",
        "            score = scoring_func(y, y_pred)\n",
        "            scores.append(score)\n",
        "\n",
        "        return pooling_func(scores)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev1QTSkwOtq_",
        "colab_type": "text"
      },
      "source": [
        "### Problem: Wie tunen wir die Hyperparameter?\n",
        "\n",
        "Problem: Unsere Idee sieht vor ein Modell auf alle Genres innerhalb einer \"Periode\" zu trainieren und auf alle anderen anzuwenden, um abzuschätzen wie sehr sich die Genres über die Zeit verändern. Hierbei stellt sich die Frage, wie man die Hyperparameter der Modelle valide und gleichzeitig effektiv optimieren kann.\n",
        "\n",
        "* Möglichkeit 1:\n",
        "    * Gridsearch auf Ausgangsperiode\n",
        "    * Vorteile:\n",
        "        * Wahrscheinlich am ehesten valide\n",
        "    * Nachteile:\n",
        "        * Unsere Datengrundlage ist zu klein, um dass für einzelne Epochen sinnvoll durchzuführen\n",
        "* Möglichkeit 2:\n",
        "    * Gridsearch auf allen Daten\n",
        "    * Vorteile:\n",
        "        * Große Datenmenge\n",
        "        * Modell würde auf alle Eigenheiten der Perioden getuned werden (wobei das eher ein Nachteil ist)\n",
        "    * Nachteil:\n",
        "        * Spätere Testdaten würden fürs Optimieren verwendet werden\n",
        "* Möglichkeit 3:\n",
        "    * ParamDict verwenden, um die den eigentlich Lauf (das Trainieren auf einer Epoche und Testen auf allen Anderen) mit allen möglichen Hyperparamtern zu testen. Eigene Evaulation (bsp. Mittelwert der F1-Scores für die verschiedenen Epochen)\n",
        "    * Vorteile:\n",
        "        * Klare Trennung von Test und Trainingsdaten\n",
        "        * Mehr Daten für die Optimierung als bei Möglichkeit 1\n",
        "    * Nachteile:\n",
        "        * keine cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJll098LOtrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNTAduy9Pht-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "d11eacef-d32c-47a6-c477-d99b2cee9151"
      },
      "source": [
        "!pip install stop_words"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32916 sha256=40a1030c0ca3eada619d064b0f4bed62c013a5c17daec602aa0d4dec59a5b4d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgoygjA4OxG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "18b8b0b0-f832-4755-e493-aaeee073b9fb"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etQ-W29_O5vE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0d5ea30-31e9-41a7-9224-7fa1e43dcef3"
      },
      "source": [
        "!ls /content/gdrive/My\\ Drive/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "full_dataset.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux6i71RfOtrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/My Drive/full_dataset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSremhOROtrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove news genre\n",
        "df = df[df.genre != 'NEWS']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWr0rCcDOtrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_p1 = df.loc[df['period'] == 'P1']\n",
        "df_rest = df.loc[df['period'] != 'P1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-NiWcaHOtrO",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGNoSRjiOtrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "tfidf = TfidfVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYXH26JxOtrU",
        "colab_type": "text"
      },
      "source": [
        "# Bauen der Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtseh1vjOtrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline, make_union, Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX5iGM3YOtrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGGzLvaPOtrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "103d9e32-8d88-42e5-cebc-ef325bbb84d1"
      },
      "source": [
        "pipe_svm = Pipeline([('tfidf', tfidf), ('linearsvc', LinearSVC(loss='hinge'))])\n",
        "pipe_svm"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('linearsvc',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "                           penalty='l2', random_state=None, tol=0.0001,\n",
              "                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGR-D6h9Otrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe_svm_params = {\n",
        "    'tfidf__max_features': [100, 500, 1000, 5000, 10000, 15000, 20000],\n",
        "    'tfidf__stop_words': [None, get_stop_words('de')],\n",
        "    'tfidf__analyzer': ['word', 'char', 'char_wb'],\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 5), (1, 5)],\n",
        "    'linearsvc__C': list(range(1,21)),\n",
        "    'linearsvc__penalty': ['l2']\n",
        "    \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIyiuscaOtrg",
        "colab_type": "text"
      },
      "source": [
        "# 1. Möglichkeit: Gridsearch auf Trainingsperiode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92CFvLD8Otri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "gridsearch = GridSearchCV(\n",
        "    pipe_svm,\n",
        "    pipe_svm_params,\n",
        "    scoring='f1_macro',\n",
        "    verbose=1,\n",
        "    n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLEMSQvoOtrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "66761d09-b988-4c5a-d8f6-1f8655e86f57"
      },
      "source": [
        "gridsearch.fit(df_p1.text, df_p1.genre.to_numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4200 candidates, totalling 21000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   58.5s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYqSbESYOtrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gridsearch.best_params_, gridsearch.best_score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u93G34xIOtro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_results = pd.DataFrame.from_dict(gridsearch.cv_results_)\n",
        "svm_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os3uwDoOOtrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}